# User-Actions-Aggregation-with-Spark

## Описание проекта
Spark приложение для вычисления агрегированных показателей за последние 7 дней. Программа обрабатывает логи действий пользователей в формате CSV и подсчитывает количество действий для каждого пользователя.

## Входные данные
Логи представляют собой CSV файлы, каждый из которых содержит записи за один день с тремя колонками:
- `email`
- `action` (один из: CREATE, READ, UPDATE, DELETE)
- `dt` (дата и время)

| email               | action | dt                  |
|---------------------|--------|---------------------|
| rAKOlO@gmail.com     | DELETE | 2024-09-10 01:38:01 |
| WSbGEouWSRK@ya.ru    | CREATE | 2024-09-10 14:15:10 |
| qeRSs@gmail.com      | CREATE | 2024-09-10 19:38:43 |
| DWYlWGvum@gmail.com  | CREATE | 2024-09-10 22:27:43 |

Для генерации входных данных использован [скрипт](https://github.com/andreyyarigin/user-actions-aggregation-test-case/blob/main/scripts/generate_sample_data.py)

## Сценарий использования
Приложение вычисляет суммарное количество каждого действия для каждого пользователя за 7 предыдущих дней. Выходной CSV файл должен содержать одну строку для каждого уникального пользователя с почтой и количествами каждого из типов действий. 

Формат выходного файла:

| email               | create_count | read_count | update_count | delete_count |
|---------------------|--------------|------------|--------------|--------------|
| DWYlWGvum@gmail.com  | 300          | 352        | 371          | 351          |
| rAKOlO@gmail.com     | 400          | 337        | 322          | 347          |
| WSbGEouWSRK@ya.ru    | 362          | 337        | 385          | 349          |
| aRrtQbOU@ya.ru       | 378          | 350        | 342          | 347          |
| qeRSs@gmail.com      | 370          | 366        | 340          | 346          |


Итоговый файл сохраняется в директорию `data/output` с именованием по правилу `YYYY-mm-dd.csv`. Например, если вычисление агрегата выполняется за 2024-09-16, то данные за период с 2024-09-09 по 2024-09-15 будут использованы, и результат будет записан в `data/output/2024-09-16.csv`.

## Реализованная функциональность
- DAG для Airflow, который ежедневно в 7:00 посредством DockerOperator запускает задачу, иницирующую запуск контейнера Spark и выполнение скрипта агрегации данных в нем:
- Скрипт выполняет преадагрегаци за каждый день из недельного диапазона и помещает их в 'data/daily_agg' (осуществляется сохранение ежедневных предагрегаций для повторного использования при построении последующих недельных агрегаций)
- Результат недельной агрегации сохраняется в папку data/ouput.

## Комментарии к структуре проекта
```
.
├── README.MD
├── docker-compose.yaml
├── spark_agg_dockerfile          # Каталог с файлами для сборки Docker-образа Spark
│   ├── Dockerfile
│   ├── requirements.txt
│   └── spark_agg_script.py       # Скрипт для выполнения агрегации данных в Spark (копируется в контейнер Spark на этапе создания и выполняется непосредственно в нем)
├── dags                          
│   └── agg_dag.py                # DAG для Airflow (Ежедневно в 7:00 посредством DockerOperator запускает задачу, которая инициирует выполнение скрипта агрегации данных в контейнере Spark)
├── data
│   ├── daily_agg                 # Предагрегации за каждый день
│   ├── input                     # Входные данные в формате CSV (логи CRUD операций)
│   │   ├── 2024-09-10.csv
│   ├── ...
│   │   └── 2024-10-16.csv
│   └── output                    # Папа для сохрарения итогового результата в формате <YYYY-MM-DD>_agg.csv 
├── input                         # Входные данные в формате CSV (логи CRUD операций)
│   ├── 2024-09-10.csv
│   ├── ...
│   └── 2024-10-09.csv
└── scripts                       
    ├── agg_script.py             # Базовый скрипт для выполнения аналогичной недельной агрегации посредством Pandas (базовый/начальный вариант выполнения агрегации - без использования Spark)
    └── generate_sample_data.py   # Скрипт для генерации тестовых входных данных (результат сохранен в data/input)
```